{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DL4H Final Project (Draft)\n",
        "\n",
        "## Graph Representation Learning for Familial Relationships\n",
        "\n",
        "[Link to original paper](https://arxiv.org/pdf/2304.05010.pdf)\n",
        "\n",
        "[Link to Github](https://github.com/dsgelab/family-EHR-graphs)\n",
        "\n",
        "[Link to replicated Github](https://github.com/cosmocat27/ehr_graph_replica/tree/main)\n",
        "\n",
        "(This notebook should be run in a python colab environment so that the setup works as expected)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "AOtc5h5Q1fBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "## Background of the problem\n",
        "\n",
        "The general problem is that of using family medical history to predict disease. Solving this problem would mean more accurate prediction and understanding of certain heritable diseases, leading to more useful medical interventions. Family history is a well-established indicator of health risks, but its assessment is often\n",
        "complicated by the interactions of genetic, environmental, and lifestyle factors. The wide\n",
        "availability of EHR presents opportunities for deep learning to learn complex representations of\n",
        "patient data that would be useful in clinical prediction.\n",
        "\n",
        "Various methods have been used to model family history and heriditary disease, including polygenic risk scoring methods and BLUP, but these approaches are limited by data availability such as genetic data. Alternatively, there are clinical baselines that use rule-based and MLP approaches that do not consider graphical information, but do not perform as well as graph based approaches.\n",
        "\n",
        "## Paper explanation\n",
        "\n",
        "This paper formulates disease risk prediction from family history as a graph modeling problem, and uses graph-based deep learning and LSTMs to learn supervised representations of the family history. It’s shown that the approach can predict 10-year disease risk better than the baseline approaches, based on AUC-ROC/PRC. Furthermore, graph explainability techniques can be used to identify specific features of the family history that are useful for disease prediction.\n",
        "\n",
        "The paper did a good job of contributing a novel graph-based method to the research regime and improving on existing performance, and it advances the state of the art on using deep learning to model heritable disease risk."
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "## Hypotheses and corresponding experiments\n",
        "\n",
        "\n",
        "1.   Hypothesis 1: The graph neural network based model provides better performance in predicting 10 year disease onset of certain diseases (adult asthma, colorectal cancer, coronary heart disease, depression and suicide, and type two diabetes), compared to the baseline (rule-based or static MLP). To test this, we will run the full GNN model and compare the AUC-ROC/PRC results with the baseline models.\n",
        "2.   Hypothesis 2: Features for family history provide incremental predictive value when encoded into a graph neural network and used to classify a patient’s development of disease within 10 years. To test this, we will run the ablation studies proposed in the paper, which incrementally add family history and graph connectivity features to a baseline model to understand their incremental value."
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "Sets up the packages, repos, etc that will be used for this colab."
      ],
      "metadata": {
        "id": "NXyVn2wdWCBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing files from public directory instead\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "!git clone https://github.com/dsgelab/family-EHR-graphs.git\n",
        "!git clone https://github.com/cosmocat27/ehr_graph_replica.git\n",
        "!pip install torch_geometric\n",
        "!mkdir results\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/family-EHR-graphs/src')\n",
        "\n",
        "### The following section is for data generation only. The result of the following code has been uploaded via Drive.\n",
        "\n",
        "#import os\n",
        "#os.environ['LD_LIBRARY_PATH'] = os.environ.get(\"LD_LIBRARY_PATH\") + \":/content/gsl-2.7/.libs\"\n",
        "\n",
        "#!wget \"https://ftp.gnu.org/gnu/gsl/gsl-2.7.tar.gz\" && tar -xvzf gsl-2.7.tar.gz\n",
        "#!cd gsl-2.7 && ./configure && make && make install\n",
        "#!gcc \"/content/drive/My Drive/project/SimPedPheno_V1.1.c\" -o PhenoPedSim -Lgsl_lib_directory -Igsl_include_directory -lm -lgsl -fPIC -lcblas -lblas\n",
        "#!./PhenoPedSim \"/content/drive/My Drive/project/syn_data_params_1.txt\" 1000"
      ],
      "metadata": {
        "id": "sfk8Zrul_E8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages you need\n",
        "import argparse\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from data import DataFetch, Data, GraphData\n",
        "from torch_geometric.loader import DataLoader\n",
        "from model import Baseline, BaselineLongitudinal, GNN, GNNLongitudinal, GNNExplainabilityLSTM\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm import tqdm\n",
        "from utils import EarlyStopping, get_classification_threshold_auc, get_classification_threshold_precision_recall, WeightedBCELoss\n",
        "import explainability\n",
        "import json\n",
        "\n",
        "from main import *\n",
        "\n",
        "# set the seed\n",
        "seed = 1000\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology"
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup the params to be used for the first experiment\n",
        "\n",
        "sqlpath = 'long.db'\n",
        "params = {'model_type':'graph',\n",
        "        'gnn_layer':'graphconv',\n",
        "        'pooling_method':'target',\n",
        "        'outpath':'results',\n",
        "        'outname':'G2_TestDisease',\n",
        "        'obs_window_start':1990,\n",
        "        'obs_window_end':2010,\n",
        "        'batchsize':250,\n",
        "        'num_workers':6,\n",
        "        'max_epochs':100,\n",
        "        'patience':8,\n",
        "        'learning_rate':0.001,\n",
        "        'main_hidden_dim':20,\n",
        "        'lstm_hidden_dim':20,\n",
        "        'loss':'bce_weighted_sum',\n",
        "        'gamma':1,\n",
        "        'alpha':1,\n",
        "        'beta':1,\n",
        "        'delta':1,\n",
        "        'dropout_rate':0.5,\n",
        "        'threshold_opt':'precision_recall',\n",
        "        'ratio':0.5,\n",
        "        'local_test':True,\n",
        "        'explainability_mode':False,\n",
        "        'embeddings_mode':False,\n",
        "        'explainer_input':'',\n",
        "        'device_specification':'na',\n",
        "        'num_positive_samples':5000}\n",
        "\n",
        "if params['device_specification'] != 'na':\n",
        "    params['device'] = torch.device(params['device_specification'])\n",
        "else:\n",
        "    params['device'] = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(\"Using {} device\".format(params['device']))"
      ],
      "metadata": {
        "id": "dqprk6SXLoyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data\n",
        "### Source of the data\n",
        "\n",
        "The experiments in the paper use a nationwide health registry dataset, which cannot be publicly\n",
        "shared for data privacy reasons. Instead, the authors have provided code and instructions for\n",
        "generating synthetic datasets that mimic the real dataset. I am using the synthetic dataset provided with the code at https://github.com/dsgelab/family-EHR-graphs/tree/main/test.\n",
        "\n",
        "The data consist of 4 different file types:\n",
        "\n",
        "**Maskfile**: Specifies which samples belong to the target cohort (patients to predict health outcomes for) and which samples belong to the graph cohort (relatives of the target patients). This file also specifies the train, validation and test split for the dataset.\n",
        "\n",
        "**Statfile**: Contains the (static node) feature dataset for all samples in both the target and graph cohorts. This file also contains the data for the label being predicted for the binary classification task.\n",
        "\n",
        "**Edgefile**: Contains the edge pairs for the family graphs, where each patient in the target cohort has a separate family graph. This file also contains the data for the edge features.\n",
        "\n",
        "**Featfile**: Specifies which features to use for training the model, for 4 types of features: static, longitudinal, label and edge."
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dir and function to load raw data\n",
        "raw_data_dir = 'family-EHR-graphs/test/'\n",
        "maskfile_path = raw_data_dir + 'Gen3_50k_0.7_142857_maskfile.csv'\n",
        "statfile_path = raw_data_dir + 'Gen3_50k_0.7_142857_statfile.csv'\n",
        "edgefile_path = raw_data_dir + 'Gen3_50k_0.7_142857_edgefile.csv'\n",
        "\n",
        "def load_raw_data(maskfile_path, statfile_path, edgefile_path):\n",
        "  masks = pd.read_csv(maskfile_path)\n",
        "  stats = pd.read_csv(statfile_path)\n",
        "  edges = pd.read_csv(edgefile_path)\n",
        "  return masks, stats, edges\n",
        "\n",
        "masks, stats, edges = load_raw_data(maskfile_path, statfile_path, edgefile_path)\n",
        "\n",
        "filepaths = {'maskfile':maskfile_path,\n",
        "            'featfile':raw_data_dir + 'featfiles/featfile_G2.csv',\n",
        "            'alt_featfile':raw_data_dir + 'featfiles/featfile_A5.csv',\n",
        "            'statfile':statfile_path,\n",
        "            'edgefile':edgefile_path}"
      ],
      "metadata": {
        "id": "0Tcqek0ql1pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistics\n",
        "\n",
        "**Size:**\n",
        "\n",
        "* Maskfile: 150k rows, 5 columns\n",
        "* Statfile: 150k rows, 33 columns\n",
        "* Edgefile: 1.1M rows, 14 columns\n",
        "* Featfile: Depends on the model, but only a few rows specifying the features.\n",
        "\n",
        "**Label distribution:**\n",
        "\n",
        "150k total patients\n",
        "\n",
        "Target population (patients): 4357 positive, 34940 negative\n",
        "\n",
        "Non-target population (relatives): 57763 negative, 52940 negative\n",
        "\n",
        "**Cross validation split:**\n",
        "\n",
        "Out of 39k target patients\n",
        "\n",
        "28k train, 4k validation, 8k test"
      ],
      "metadata": {
        "id": "DrITplYrrGp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate statistics\n",
        "def calculate_stats(masks, stats, edges):\n",
        "  n_rows, n_columns = masks.shape\n",
        "  print(\"Maskfile size: {} rows, {} columns\".format(n_rows, n_columns))\n",
        "  n_rows, n_columns = stats.shape\n",
        "  print(\"Statfile size: {} rows, {} columns\".format(n_rows, n_columns))\n",
        "  n_rows, n_columns = edges.shape\n",
        "  print(\"Edgefile size: {} rows, {} columns\".format(n_rows, n_columns))\n",
        "\n",
        "  target_pos, target_neg = stats[masks['target']==1].EndPtStat.value_counts().sort_index().values\n",
        "  print(\"Target population: {} positive, {} negative\".format(target_pos, target_neg))\n",
        "  nontarget_pos, nontarget_neg = stats[masks['target']==0].EndPtStat.value_counts().sort_index().values\n",
        "  print(\"Non-target population: {} positive, {} negative\".format(nontarget_pos, nontarget_neg))\n",
        "  nontarget, train, valid, test = masks.train.value_counts().sort_index().values\n",
        "  print(\"Cross validation split: {} train, {} validation, {} test\".format(train, valid, test))\n",
        "  return None\n",
        "\n",
        "calculate_stats(masks, stats, edges)"
      ],
      "metadata": {
        "id": "HH3KiBfoM3bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data processing\n",
        "\n",
        "Most of the data is processed already via data generation. We use the provided function DataFetch to split the data into train / validation / test sets, then use get_data_and_loader to prepare the datasets for modeling."
      ],
      "metadata": {
        "id": "7IPQ_It-rPaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fetch_data = DataFetch(filepaths['maskfile'], filepaths['featfile'], filepaths['statfile'], filepaths['edgefile'], sqlpath, params, alt_featfile=filepaths['alt_featfile'], local=params['local_test'])\n",
        "\n",
        "train_patient_list = fetch_data.train_patient_list\n",
        "params['num_batches_train'] = int(np.ceil(len(train_patient_list)/params['batchsize']))\n",
        "params['num_samples_train_dataset'] = len(fetch_data.train_patient_list)\n",
        "params['num_samples_train_minority_class'] = fetch_data.num_samples_train_minority_class\n",
        "params['num_samples_train_majority_class'] = fetch_data.num_samples_train_majority_class\n",
        "validate_patient_list = fetch_data.validate_patient_list\n",
        "params['num_batches_validate'] = int(np.ceil(len(validate_patient_list)/params['batchsize']))\n",
        "params['num_samples_valid_dataset'] = len(fetch_data.validate_patient_list)\n",
        "params['num_samples_valid_minority_class'] = fetch_data.num_samples_valid_minority_class\n",
        "params['num_samples_valid_majority_class'] = fetch_data.num_samples_valid_majority_class\n",
        "test_patient_list = fetch_data.test_patient_list\n",
        "params['num_batches_test'] = int(np.ceil(len(test_patient_list)/params['batchsize']))\n",
        "\n",
        "train_dataset, train_loader = get_data_and_loader(train_patient_list, fetch_data, params, shuffle=True)\n",
        "validate_dataset, validate_loader = get_data_and_loader(validate_patient_list, fetch_data, params, shuffle=True)\n",
        "test_dataset, test_loader = get_data_and_loader(test_patient_list, fetch_data, params, shuffle=False)\n",
        "params['include_longitudinal'] = train_dataset.include_longitudinal\n",
        "params['num_features_static'] = len(fetch_data.static_features)\n",
        "if params['model_type'] in ['graph', 'graph_no_target', 'explainability']: params['num_features_alt_static'] = len(fetch_data.alt_static_features)\n",
        "params['num_features_longitudinal'] = len(fetch_data.longitudinal_features)"
      ],
      "metadata": {
        "id": "aiaHp9SdKk2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Model\n",
        "### Model architecture\n",
        "The GNN Longitudinal model is a graph embedding model that consists of two separate paths for family-based data and patient-based data, which get combined into final classification.\n",
        "\n",
        "The patient part consists of LSTM, Linear, and Dropout layers with ReLU activation.\n",
        "\n",
        "The family part consists of LSTM and graph convolutional layers (with GCN), with ReLU activation.\n",
        "\n",
        "The combined part concatenates the patient and family outputs and passes them through a linear layer and sigmoid activation function to get the final output.\n",
        "\n",
        "### Training objectives\n",
        "Loss function: WeightedBCELoss (BCELoss with weights adjusted for class imbalance)\n",
        "\n",
        "Optimizer: Adam with learning rate = 0.001\n",
        "\n",
        "### Others\n",
        "The model was trained on the synthetic dataset, we are loading the trained version of the model for expediency (but running 1 iteration of training for demonstration)."
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "l_JwQDtZiNhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model(params)\n",
        "model_path = '{}/{}_model.pth'.format(params['outpath'], params['outname'])\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
        "if params['loss']=='bce_weighted_single' or params['loss']=='bce_weighted_sum':\n",
        "  train_criterion = WeightedBCELoss(params['num_samples_train_dataset'], params['num_samples_train_minority_class'], params['num_samples_train_majority_class'], params['device'])\n",
        "  valid_criterion = WeightedBCELoss(params['num_samples_valid_dataset'], params['num_samples_valid_minority_class'], params['num_samples_valid_majority_class'], params['device'])\n",
        "\n",
        "# normal training model\n",
        "del fetch_data # free up memory no longer needed\n",
        "del train_dataset\n",
        "del validate_dataset\n",
        "del test_dataset\n",
        "\n",
        "# model training (we run 1 epoch just for demonstration)\n",
        "params['max_epochs'] = 1\n",
        "start_time_train = time.time()\n",
        "# train the model for at most max_epochs\n",
        "model, threshold = train_model(model, train_loader, validate_loader, params)\n",
        "end_time_train = time.time()\n",
        "# we will load the pretrained model\n",
        "#torch.save(model.state_dict(), model_path)\n",
        "threshold = 0.639   # threshold determined from training\n",
        "params['threshold'] = threshold\n",
        "params['training_time'] = end_time_train - start_time_train"
      ],
      "metadata": {
        "id": "gBdVZoTvsSFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Testing"
      ],
      "metadata": {
        "id": "CJY2QPg3iRho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'ehr_graph_replica/results_G2/G2_TestDisease_model.pth'\n",
        "results_path = '{}/{}_results.csv'.format(params['outpath'], params['outname'])\n",
        "stats_path = '{}/{}_stats.csv'.format(params['outpath'], params['outname'])\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# model testing\n",
        "results, metric_results = test_model(model, test_loader, threshold, params)\n",
        "results.to_csv(results_path, index=None)\n",
        "params.update(metric_results)\n",
        "stats = pd.DataFrame({'name':list(params.keys()), 'value':list(params.values())})\n",
        "stats.to_csv(stats_path, index=None)"
      ],
      "metadata": {
        "id": "WV9zPq6Y-QZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "\n",
        "This section summarizes the results of the models we have tested. The main model from the paper is the GNN model with longitudinal data. In addition, we provide results of the other models, the results of which we load directly:\n",
        "\n",
        "* Baseline model with age and sex data\n",
        "* Age, sex, and family history MLP\n",
        "* Age, sex, and graph connectivity MLP"
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GNN model with longitudinal data\n",
        "\n",
        "Test set size: 7860\n",
        "\n",
        "Accuracy: 0.77\n",
        "\n",
        "Recall: 0.64\n",
        "\n",
        "Precision: 0.28\n",
        "\n",
        "F1 Score: 0.388\n",
        "\n",
        "ROC_AUC: 0.803"
      ],
      "metadata": {
        "id": "8VxsqW51lqOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Figure 1: Loss shrinkage for GNN model**\n",
        "\n",
        "![sample_image.png](https://drive.google.com/uc?export=view&id=1D3T-qjw5mYLfL8h5X9NwEVT2w3IDtbSI)"
      ],
      "metadata": {
        "id": "sxIp0b-D3Pbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the results directly if we did not run the model in the previous step\n",
        "#results = pd.read_csv('ehr_graph_replica/results_G2/G2_TestDisease_results.csv')\n",
        "#stats = pd.read_csv('ehr_graph_replica/results_A2/G2_TestDisease_stats.csv')\n",
        "\n",
        "y_true = results['actual']\n",
        "y_score = results['pred_raw']\n",
        "y_pred = results['pred_binary']\n",
        "\n",
        "# metrics to evaluate my model\n",
        "print(\"Accuracy: \", round(metrics.accuracy_score(y_true, y_pred), 3))\n",
        "print(\"Recall: \", round(metrics.recall_score(y_true, y_pred), 3))\n",
        "print(\"Precision: \", round(metrics.precision_score(y_true, y_pred), 3))\n",
        "print(\"F1: \", round(metrics.f1_score(y_true, y_pred), 3))\n",
        "print(\"ROC AUC: \", round(metrics.roc_auc_score(y_true, y_score), 3))"
      ],
      "metadata": {
        "id": "4Jppkpn5rXrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline model with age and sex data\n",
        "\n",
        "Test set size: 7860\n",
        "\n",
        "Accuracy: 0.65\n",
        "\n",
        "Recall: 0.51\n",
        "\n",
        "Precision: 0.16\n",
        "\n",
        "F1 Score: 0.243\n",
        "\n",
        "ROC_AUC: 0.626"
      ],
      "metadata": {
        "id": "8SWiC-NCmqxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the results directly (we trained and ran the model separately)\n",
        "results = pd.read_csv('ehr_graph_replica/results_A1/A1_TestDisease_results.csv')\n",
        "stats = pd.read_csv('ehr_graph_replica/results_A1/A1_TestDisease_stats.csv')\n",
        "\n",
        "y_true = results['actual']\n",
        "y_score = results['pred_raw']\n",
        "y_pred = results['pred_binary']\n",
        "\n",
        "# metrics to evaluate my model\n",
        "print(\"Accuracy: \", round(metrics.accuracy_score(y_true, y_pred), 3))\n",
        "print(\"Recall: \", round(metrics.recall_score(y_true, y_pred), 3))\n",
        "print(\"Precision: \", round(metrics.precision_score(y_true, y_pred), 3))\n",
        "print(\"F1: \", round(metrics.f1_score(y_true, y_pred), 3))\n",
        "print(\"ROC AUC: \", round(metrics.roc_auc_score(y_true, y_score), 3))"
      ],
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Age, sex and family history MLP\n",
        "\n",
        "Test set size: 7860\n",
        "\n",
        "Accuracy: 0.73\n",
        "\n",
        "Recall: 0.63\n",
        "\n",
        "Precision: 0.24\n",
        "\n",
        "F1 Score: 0.344\n",
        "\n",
        "ROC_AUC: 0.768"
      ],
      "metadata": {
        "id": "Qe5ghejZxhDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the results directly (we trained and ran the model separately)\n",
        "results = pd.read_csv('ehr_graph_replica/results_A2/A2_TestDisease_results.csv')\n",
        "stats = pd.read_csv('ehr_graph_replica/results_A2/A2_TestDisease_stats.csv')\n",
        "\n",
        "y_true = results['actual']\n",
        "y_score = results['pred_raw']\n",
        "y_pred = results['pred_binary']\n",
        "\n",
        "# metrics to evaluate my model\n",
        "print(\"Accuracy: \", round(metrics.accuracy_score(y_true, y_pred), 3))\n",
        "print(\"Recall: \", round(metrics.recall_score(y_true, y_pred), 3))\n",
        "print(\"Precision: \", round(metrics.precision_score(y_true, y_pred), 3))\n",
        "print(\"F1: \", round(metrics.f1_score(y_true, y_pred), 3))\n",
        "print(\"ROC AUC: \", round(metrics.roc_auc_score(y_true, y_score), 3))"
      ],
      "metadata": {
        "id": "5E9nAvMHvZTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Age, sex and graph connectivity MLP\n",
        "\n",
        "Test set size: 7860\n",
        "\n",
        "Accuracy: 0.67\n",
        "\n",
        "Recall: 0.49\n",
        "\n",
        "Precision: 0.17\n",
        "\n",
        "F1 Score: 0.249\n",
        "\n",
        "ROC_AUC: 0.638"
      ],
      "metadata": {
        "id": "sT-aNvoExx6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the results directly (we trained and ran the model separately)\n",
        "results = pd.read_csv('ehr_graph_replica/results_A3/A3_TestDisease_results.csv')\n",
        "stats = pd.read_csv('ehr_graph_replica/results_A3/A3_TestDisease_stats.csv')\n",
        "\n",
        "y_true = results['actual']\n",
        "y_score = results['pred_raw']\n",
        "y_pred = results['pred_binary']\n",
        "\n",
        "# metrics to evaluate my model\n",
        "print(\"Accuracy: \", round(metrics.accuracy_score(y_true, y_pred), 3))\n",
        "print(\"Recall: \", round(metrics.recall_score(y_true, y_pred), 3))\n",
        "print(\"Precision: \", round(metrics.precision_score(y_true, y_pred), 3))\n",
        "print(\"F1: \", round(metrics.f1_score(y_true, y_pred), 3))\n",
        "print(\"ROC AUC: \", round(metrics.roc_auc_score(y_true, y_score), 3))"
      ],
      "metadata": {
        "id": "Ls4Rotpuz0da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis and Plans\n",
        "\n",
        "The results suggest that the Graph-based model with longitudinal data outperforms non graph-based models when making predictions on the synthetic dataset. They also suggset that family history plays a significant role in the prediction of the disease, since those models performed significantly better.\n",
        "\n",
        "In further work, we could try different combinations of features or look into how the models perform on datasets with different characteristics like heritability to see if the pattern continues to hold."
      ],
      "metadata": {
        "id": "WA5XLrPd6NdB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model comparison"
      ],
      "metadata": {
        "id": "8EAWAy_LwHlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results of models in the paper for predicting coronary heart disease (AUC-ROC):\n",
        "\n",
        "Baseline model with age and sex data: 0.696\n",
        "\n",
        "Age, sex and family history MLP: 0.710\n",
        "\n",
        "Age, sex and graph connectivity MLP: 0.696\n",
        "\n",
        "GNN model with longitudinal data: 0.775\n",
        "\n",
        "### Results of models in replicated experiment for predicting on synthetic dataset (AUC-ROC):\n",
        "\n",
        "Baseline model with age and sex data: 0.626\n",
        "\n",
        "Age, sex and family history MLP: 0.768\n",
        "\n",
        "Age, sex and graph connectivity MLP: 0.638\n",
        "\n",
        "GNN model with longitudinal data: 0.803"
      ],
      "metadata": {
        "id": "ZBXjPw_10bU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "Based on the experiment, we were able to reproduce two of the major findings in the paper, which were 1) that graph based representation learning based on family history provides better preditive value over non-graph based models, and 2) family history is a significant factor in how well the model performs, since all models that included family history performed significantly better than those that did not.\n",
        "\n",
        "Of course, we should note the caveat that this was using synthetic data instead of the real dataset, which may yield different results. This is probably the major contributing gap between the original results and the replicated results.\n",
        "\n",
        "The easy part was that most of the code for producing the data and training the models was available in the Github, making it relatively easy for someone to run similar code in their own environment. The difficult part is that it's not easy to compare the results of the new dataset with those of the original dataset, due to their different sources.\n",
        "\n",
        "The authors might consider going into detail about some methods that use less sensitive data that can be accessed more widely, even though it is not optimal, so that follow-up experiments can compared using the same datasets."
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1.   Sophie Wharrie, Zhiyu Yang, Andrea Ganna, Samuel Kaski. (2023). Characterizing personalized\n",
        "effects of family information on disease risk using graph representation learning. Proceedings of\n",
        "the 8th Machine Learning for Healthcare Conference, in Proceedings of Machine Learning\n",
        "Research. 219:824-845. Available from https://arxiv.org/abs/2304.05010.\n",
        "\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    }
  ]
}